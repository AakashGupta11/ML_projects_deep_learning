{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2454,
     "status": "ok",
     "timestamp": 1533301472673,
     "user": {
      "displayName": "Aakash Gupta",
      "photoUrl": "//lh6.googleusercontent.com/-kX-2bxhvVVI/AAAAAAAAAAI/AAAAAAAAABY/Nc0rPjwkC8s/s50-c-k-no/photo.jpg",
      "userId": "116951462494679469142"
     },
     "user_tz": -330
    },
    "id": "gTXnwPKLJGyS",
    "outputId": "ba0c9b16-1f24-4ce5-9686-af5abed041e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAKASH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\AAKASH\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CUvjQkDKIdov"
   },
   "outputs": [],
   "source": [
    "filename = 'paragraph.txt'\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(raw_text)\n",
    "punc = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc.remove('.')\n",
    "punc += ['‘', '’', '--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_words = []\n",
    "for word in all_words:\n",
    "    if(word not in punc):\n",
    "        pure_words.append(word)\n",
    "text = ' '.join(pure_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"samuel butler has a lot to answer for . but for him a modern traveler could spend his time peacefully admiring the scenery instead of feeling himself bound to dog the simple and grotesque of the world for the sake of their too-human comments . it is his fault if a peasant 's _naïveté_ has come to outweigh the beauty of rivers and the remarks of clergymen are more than mountains . it is very restful to give up all effort at observing human nature and drawing social and political deductions from trifles and to let oneself relapse into wide-mouthed worship of the wonders of nature . and this is very easy at niagara . niagara means nothing . it is not leading anywhere . it does not result from anything . it throws no light on the effects of protection nor on the facility for divorce in america nor on corruption in public life nor on canadian character nor even on the navy bill . it is merely a great deal of water falling over some cliffs . but it is very remarkably that . the human race apt as a child to destroy what it admires has done its best to surround the falls with every distraction incongruity and vulgarity . hotels powerhouses bridges trams picture post-cards sham legends stalls booths rifle-galleries and side-shows frame them about . and there are touts . niagara is the central home and breeding-place for all the touts of earth . there are touts insinuating and touts raucous greasy touts brazen touts and upper-class refined gentlemanly take-you-by-the-arm touts touts who intimidate and touts who wheedle professionals amateurs and _dilettanti_ male and female touts who would photograph you with your arm round a young lady against a faked background of the sublimest cataract touts who would bully you into cars char-à-bancs elevators or tunnels or deceive you into a carriage and pair touts who would sell you picture post-cards moccasins sham indian beadwork blankets tee-pees and crockery and touts finally who have no apparent object in the world but just purely simply merely incessantly indefatigably and ineffugibly to tout . and in the midst of all this overwhelming it all are the falls . he who sees them instantly forgets humanity . they are not very high but they are overpowering . they are divided by an island into two parts the canadian and the american . half a mile or so above the falls on either side the water of the great stream begins to run more swiftly and in confusion . it descends with ever-growing speed . it begins chattering and leaping breaking into a thousand ripples throwing up joyful fingers of spray . sometimes it is divided by islands and rocks sometimes the eye can see nothing but a waste of laughing springing foamy waves turning crossing even seeming to stand for an instant erect but always borne impetuously forward like a crowd of triumphant feasters . sit close down by it and you see a fragment of the torrent against the sky mottled steely and foaming leaping onward in far-flung criss-cross strands of water . perpetually the eye is on the point of descrying a pattern in this weaving and perpetually it is cheated by change . in one place part of the flood plunges over a ledge a few feet high and a quarter of a mile or so long in a uniform and stable curve . it gives an impression of almost military concerted movement grown suddenly out of confusion . but it is swiftly lost again in the multitudinous tossing merriment . here and there a rock close to the surface is marked by a white wave that faces backwards and seems to be rushing madly up-stream but is really stationary in the headlong charge . but for these signs of reluctance the waters seem to fling themselves on with some foreknowledge of their fate in an ever wilder frenzy . but it is no maeterlinckian prescience . they prove rather that greek belief that the great crashes are preceded by a louder merriment and a wilder gaiety . leaping in the sunlight careless entwining clamorously joyful the waves riot on towards the verge . but there they change . as they turn to the sheer descent the white and blue and slate color in the heart of the canadian falls at least blend and deepen to a rich wonderful luminous green . on the edge of disaster the river seems to gather herself to pause to lift a head noble in ruin and then with a slow grandeur to plunge into the eternal thunder and white chaos below . where the stream runs shallower it is a kind of violet color but both violet and green fray and frill to white as they fall . the mass of water striking some ever-hidden base of rock leaps up the whole two hundred feet again in pinnacles and domes of spray . the spray falls back into the lower river once more all but a little that fines to foam and white mist which drifts in layers along the air graining it and wanders out on the wind over the trees and gardens and houses and so vanishes . the manager of one of the great power-stations on the banks of the river above the falls told me that the center of the riverbed at the canadian falls is deep and of a saucer shape . so it may be possible to fill this up to a uniform depth and divert a lot of water for the power-houses . and this he said would supply the need for more power which will certainly soon arise without taking away from the beauty of niagara . this is a handsome concession of the utilitarians to ordinary sight-seers . yet i doubt if we shall be satisfied . the real secret of the beauty and terror of the falls is not their height or width but the feeling of colossal power and of unintelligible disaster caused by the plunge of that vast body of water . if that were taken away there would be little visible change but the heart would be gone . the american falls do not inspire this feeling in the same way as the canadian . it is because they are less in volume and because the water does not fall so much into one place . by comparison their beauty is almost delicate and fragile . they are extraordinarily level one long curtain of lacework and woven foam . seen from opposite when the sun is on them they are blindingly white and the clouds of spray show dark against them . with both falls the color of the water is the ever-altering wonder . greens and blues purples and whites melt into one another fade and come again and change with the changing sun . sometimes they are as richly diaphanous as a precious stone and glow from within with a deep inexplicable light . sometimes the white intricacies of dropping foam become opaque and creamy . and always there are the rainbows . if you come suddenly upon the falls from above a great double rainbow very vivid spanning the extent of spray from top to bottom is the first thing you see . if you wander along the cliff opposite a bow springs into being in the american falls accompanies you courteously on your walk dwindles and dies as the mist ends and awakens again as you reach the canadian tumult . and the bold traveler who attempts the trip under the american falls sees when he dare open his eyes to anything tiny baby rainbows some four or five yards in span leaping from rock to rock among the foam and gamboling beside him barely out of hand 's reach as he goes . one i saw in that place was a complete circle such as i have never seen before and so near that i could put my foot on it . it is a terrifying journey beneath and behind the falls . the senses are battered and bewildered by the thunder of the water and the assault of wind and spray or rather the sound is not of falling water but merely of falling a noise of unspecified ruin . so if you are close behind the endless clamor the sight can not recognize liquid in the masses that hurl past . you are dimly and pitifully aware that sheets of light and darkness are falling in great curves in front of you . dull omnipresent foam washes the face . farther away in the roar and hissing clouds of spray seem literally to slide down some invisible plane of air . beyond the foot of the falls the river is like a slipping floor of marble green with veins of dirty white made by the scum that was foam . it slides very quietly and slowly down for a mile or two sullenly exhausted . then it turns to a dull sage green and hurries more swiftly smooth and ominous . as the walls of the ravine close in trouble stirs and the waters boil and eddy . these are the lower rapids a sight more terrifying than the falls because less intelligible . close in its bands of rock the river surges tumultuously forward writhing and leaping as if inspired by a demon . it is pressed by the straits into a visibly convex form . great planes of water slide past . sometimes it is thrown up into a pinnacle of foam higher than a house or leaps with incredible speed from the crest of one vast wave to another along the shining curve between like the spring of a wild beast . its motion continually suggests muscular action . the power manifest in these rapids moves one with a different sense of awe and terror from that of the falls . here the inhuman life and strength are spontaneous active almost resolute masculine vigor compared with the passive gigantic power female helpless and overwhelming of the falls . a place of fear . one is drawn back strangely to a contemplation of the falls at every hour and especially by night when the cloud of spray becomes an immense visible ghost straining and wavering high above the river white and pathetic and translucent . the victorian lies very close below the surface in every man . there one can sit and let great cloudy thoughts of destiny and the passage of empires drift through the mind for such dreams are at home by niagara . i could not get out of my mind the thought of a friend who said that the rainbows over the falls were like the arts and beauty and goodness with regard to the stream of life caused by it thrown upon its spray but unable to stay or direct or affect it and ceasing when it ceased . in all comparisons that rise in the heart the river with its multitudinous waves and its single current likens itself to a life whether of an individual or of a community . a man 's life is of many flashing moments and yet one stream a nation 's flows through all its citizens and yet is more than they . in such places one is aware with an almost insupportable and yet comforting certitude that both men and nations are hurried onwards to their ruin or ending as inevitably as this dark flood . some go down to it unreluctant and meet it like the river not without nobility . and as incessant as inevitable and as unavailing as the spray that hangs over the falls is the white cloud of human crying ... . with some such thoughts does the platitudinous heart win from the confusion and thunder of a niagara peace that the quietest plains or most stable hills can never give .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Gu_P4Om1IdmK"
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentence_words = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    sentence_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sDnjjx-nIdfz"
   },
   "outputs": [],
   "source": [
    "all_words = word_tokenize(text)\n",
    "n_words = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1533301478377,
     "user": {
      "displayName": "Aakash Gupta",
      "photoUrl": "//lh6.googleusercontent.com/-kX-2bxhvVVI/AAAAAAAAAAI/AAAAAAAAABY/Nc0rPjwkC8s/s50-c-k-no/photo.jpg",
      "userId": "116951462494679469142"
     },
     "user_tz": -330
    },
    "id": "l2H7XpNpKzjQ",
    "outputId": "ccfd1ea5-97a9-46b2-fde1-9d175d1423eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAKASH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentence_words, min_count = 1)\n",
    "unique_words = list(model.wv.vocab)\n",
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5112223e-03, -1.1111058e-03,  9.3224703e-04,  1.2056561e-03,\n",
       "        2.7394752e-04, -4.5081587e-03, -5.4597511e-04,  2.5315809e-03,\n",
       "       -1.0564079e-03, -4.3983051e-05, -1.1389211e-03, -2.2470227e-03,\n",
       "       -1.8412161e-04,  2.2941332e-03,  4.0915054e-03,  3.7332824e-03,\n",
       "        8.3137007e-04, -2.3677328e-03, -9.7633027e-05, -1.8823985e-03,\n",
       "       -4.5288387e-03, -2.4552969e-04, -4.9671303e-03, -4.0496951e-03,\n",
       "        3.1774545e-03, -4.0811263e-03,  1.2995029e-03,  1.2434681e-03,\n",
       "        3.3467337e-03,  1.9549283e-03,  1.3234899e-03,  4.3767408e-04,\n",
       "        2.7604424e-03,  4.2024348e-03,  2.3399284e-03,  1.8831862e-03,\n",
       "       -1.9913029e-03,  2.0207732e-03, -1.0471378e-03,  4.4320053e-03,\n",
       "        1.1498063e-03, -3.7819743e-03, -3.5451443e-04, -3.4483885e-03,\n",
       "        4.3857060e-03, -4.6761392e-04,  5.9742632e-04, -4.6755769e-03,\n",
       "        1.0845689e-03,  3.8723983e-03,  5.9050263e-04, -3.2788974e-03,\n",
       "       -2.2682389e-03, -2.7858422e-03,  3.0881655e-03, -3.1968367e-03,\n",
       "       -1.4767767e-03,  1.3470764e-03,  1.9042360e-03, -3.0065170e-03,\n",
       "        3.5879263e-03,  3.0028115e-03, -4.4679795e-03, -4.5093582e-03,\n",
       "        4.3279375e-03, -3.6117985e-04,  5.2360870e-04,  4.3861414e-03,\n",
       "       -1.8322410e-03,  4.5925789e-03,  3.0870750e-03, -1.0042450e-03,\n",
       "        4.2124074e-03,  9.3764538e-05,  3.5139706e-03,  3.6001459e-03,\n",
       "       -4.9547562e-03,  1.0280714e-03,  2.0853127e-03, -6.9543201e-04,\n",
       "        1.8614137e-03,  4.7315960e-03,  4.3815100e-03, -1.3959379e-04,\n",
       "        3.4543790e-03, -1.3107626e-03,  3.4072837e-03, -4.0709213e-03,\n",
       "        4.1743233e-03,  3.2450678e-03,  3.8810305e-03,  4.8288368e-03,\n",
       "       -2.1079057e-03,  4.2389464e-03,  2.2837545e-03,  4.1060075e-03,\n",
       "       -2.0629354e-03,  3.5599098e-03,  2.5251387e-03,  1.3286028e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nR8RIxy_TnTZ"
   },
   "outputs": [],
   "source": [
    "k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NoQDI_FzKzg8"
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = k)\n",
    "# result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "42Be3W31KzcZ"
   },
   "outputs": [],
   "source": [
    "word_to_vec = {}\n",
    "word_to_int = {}\n",
    "# vec_to_word = {}\n",
    "int_to_word = {}\n",
    "int_to_vec = {}\n",
    "for i in range(len(unique_words)):\n",
    "    word_to_vec[unique_words[i]] = result[i]\n",
    "    word_to_int[unique_words[i]] = i\n",
    "#     vec_to_word[result[i][0]] = unique_words[i]\n",
    "    int_to_word[i] = unique_words[i]\n",
    "    int_to_vec[i] = result[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9o3tGCotKzZj"
   },
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(n_words - seq_length):\n",
    "    seq_in = all_words[i : i + seq_length]\n",
    "    seq_out = all_words[i + seq_length]\n",
    "    data_x = [word_to_vec[word] for word in seq_in]\n",
    "    data_y = [word_to_int[seq_out]]\n",
    "    dataX.append(data_x)\n",
    "    dataY.append(data_y)\n",
    "data_points = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yayD_Xc8LGHE"
   },
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (data_points, seq_length, k))\n",
    "Y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TWJfdz3vLGDG"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OzQ_R5YjLNE4"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10965
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91083,
     "status": "ok",
     "timestamp": 1533310456333,
     "user": {
      "displayName": "Aakash Gupta",
      "photoUrl": "//lh6.googleusercontent.com/-kX-2bxhvVVI/AAAAAAAAAAI/AAAAAAAAABY/Nc0rPjwkC8s/s50-c-k-no/photo.jpg",
      "userId": "116951462494679469142"
     },
     "user_tz": -330
    },
    "id": "UArrFFbJVCDy",
    "outputId": "94e806d1-8c23-4caf-9077-64c895388e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1993/1993 [==============================] - 11s 6ms/step - loss: 0.2076 - acc: 0.9313\n",
      "Epoch 2/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2311 - acc: 0.9247\n",
      "Epoch 3/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2252 - acc: 0.9308\n",
      "Epoch 4/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1677 - acc: 0.9398\n",
      "Epoch 5/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2140 - acc: 0.9288\n",
      "Epoch 6/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1916 - acc: 0.9418\n",
      "Epoch 7/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1955 - acc: 0.9353\n",
      "Epoch 8/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1789 - acc: 0.9428\n",
      "Epoch 9/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1866 - acc: 0.9453\n",
      "Epoch 10/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2008 - acc: 0.9373\n",
      "Epoch 11/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1938 - acc: 0.9353\n",
      "Epoch 12/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2111 - acc: 0.9298\n",
      "Epoch 13/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2278 - acc: 0.9277\n",
      "Epoch 14/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1926 - acc: 0.9403\n",
      "Epoch 15/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1694 - acc: 0.9433\n",
      "Epoch 16/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1813 - acc: 0.9398\n",
      "Epoch 17/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2337 - acc: 0.9267\n",
      "Epoch 18/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1912 - acc: 0.9418\n",
      "Epoch 19/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1939 - acc: 0.9368\n",
      "Epoch 20/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1870 - acc: 0.9423\n",
      "Epoch 21/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1856 - acc: 0.9413\n",
      "Epoch 22/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2395 - acc: 0.9162\n",
      "Epoch 23/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1859 - acc: 0.9403\n",
      "Epoch 24/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1771 - acc: 0.9453\n",
      "Epoch 25/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1769 - acc: 0.9423\n",
      "Epoch 26/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1735 - acc: 0.9393\n",
      "Epoch 27/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2000 - acc: 0.9343\n",
      "Epoch 28/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2153 - acc: 0.9262\n",
      "Epoch 29/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2753 - acc: 0.9092\n",
      "Epoch 30/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2214 - acc: 0.9303\n",
      "Epoch 31/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2761 - acc: 0.9067\n",
      "Epoch 32/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1845 - acc: 0.9368\n",
      "Epoch 33/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1570 - acc: 0.9463\n",
      "Epoch 34/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1971 - acc: 0.9318\n",
      "Epoch 35/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1874 - acc: 0.9403\n",
      "Epoch 36/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1558 - acc: 0.9523\n",
      "Epoch 37/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2212 - acc: 0.9293\n",
      "Epoch 38/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2237 - acc: 0.9282\n",
      "Epoch 39/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2006 - acc: 0.9333\n",
      "Epoch 40/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1993 - acc: 0.9313\n",
      "Epoch 41/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1953 - acc: 0.9373\n",
      "Epoch 42/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2523 - acc: 0.9227\n",
      "Epoch 43/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2812 - acc: 0.9122\n",
      "Epoch 44/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1861 - acc: 0.9353\n",
      "Epoch 45/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1596 - acc: 0.9448\n",
      "Epoch 46/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1953 - acc: 0.9318\n",
      "Epoch 47/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1858 - acc: 0.9408\n",
      "Epoch 48/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1759 - acc: 0.9413\n",
      "Epoch 49/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1767 - acc: 0.9398\n",
      "Epoch 50/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1770 - acc: 0.9428\n",
      "Epoch 51/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1723 - acc: 0.9453\n",
      "Epoch 52/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2027 - acc: 0.9308\n",
      "Epoch 53/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1832 - acc: 0.9383\n",
      "Epoch 54/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1814 - acc: 0.9408\n",
      "Epoch 55/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2232 - acc: 0.9333\n",
      "Epoch 56/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2048 - acc: 0.9333\n",
      "Epoch 57/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2105 - acc: 0.9318\n",
      "Epoch 58/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1922 - acc: 0.9358\n",
      "Epoch 59/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2019 - acc: 0.9348\n",
      "Epoch 60/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1769 - acc: 0.9423\n",
      "Epoch 61/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1830 - acc: 0.9388\n",
      "Epoch 62/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1644 - acc: 0.9458\n",
      "Epoch 63/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1595 - acc: 0.9513\n",
      "Epoch 64/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2029 - acc: 0.9338\n",
      "Epoch 65/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2457 - acc: 0.9267\n",
      "Epoch 66/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2877 - acc: 0.9132\n",
      "Epoch 67/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1803 - acc: 0.9423\n",
      "Epoch 68/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1882 - acc: 0.9383\n",
      "Epoch 69/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1708 - acc: 0.9423\n",
      "Epoch 70/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1795 - acc: 0.9443\n",
      "Epoch 71/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1656 - acc: 0.9433A: 0s - loss: 0.1614 - a\n",
      "Epoch 72/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1790 - acc: 0.9453\n",
      "Epoch 73/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1999 - acc: 0.9398\n",
      "Epoch 74/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1680 - acc: 0.9473\n",
      "Epoch 75/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2480 - acc: 0.9267\n",
      "Epoch 76/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2185 - acc: 0.9293\n",
      "Epoch 77/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2122 - acc: 0.9277\n",
      "Epoch 78/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1898 - acc: 0.9353\n",
      "Epoch 79/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1907 - acc: 0.9408\n",
      "Epoch 80/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2027 - acc: 0.9318\n",
      "Epoch 81/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1813 - acc: 0.9348\n",
      "Epoch 82/200\n",
      "1993/1993 [==============================] - 9s 4ms/step - loss: 0.1789 - acc: 0.9338\n",
      "Epoch 83/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1548 - acc: 0.9488\n",
      "Epoch 84/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2207 - acc: 0.9288\n",
      "Epoch 85/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1805 - acc: 0.9433\n",
      "Epoch 86/200\n",
      "1993/1993 [==============================] - 9s 5ms/step - loss: 0.2012 - acc: 0.9363\n",
      "Epoch 87/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1797 - acc: 0.9478\n",
      "Epoch 88/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2145 - acc: 0.9318\n",
      "Epoch 89/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2225 - acc: 0.9247\n",
      "Epoch 90/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2504 - acc: 0.9262\n",
      "Epoch 91/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2003 - acc: 0.9338\n",
      "Epoch 92/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1713 - acc: 0.9418\n",
      "Epoch 93/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1863 - acc: 0.9388\n",
      "Epoch 94/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1573 - acc: 0.9513\n",
      "Epoch 95/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1717 - acc: 0.9428\n",
      "Epoch 96/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1771 - acc: 0.9438\n",
      "Epoch 97/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1928 - acc: 0.9358\n",
      "Epoch 98/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1524 - acc: 0.9508\n",
      "Epoch 99/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2013 - acc: 0.9393\n",
      "Epoch 100/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1514 - acc: 0.9463\n",
      "Epoch 101/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1988 - acc: 0.9328\n",
      "Epoch 102/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2323 - acc: 0.9318\n",
      "Epoch 103/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1563 - acc: 0.9478\n",
      "Epoch 104/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1892 - acc: 0.9403\n",
      "Epoch 105/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2536 - acc: 0.9197\n",
      "Epoch 106/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1637 - acc: 0.9513\n",
      "Epoch 107/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1731 - acc: 0.9403\n",
      "Epoch 108/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1704 - acc: 0.9398\n",
      "Epoch 109/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1606 - acc: 0.9468\n",
      "Epoch 110/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1755 - acc: 0.9428\n",
      "Epoch 111/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1792 - acc: 0.9428\n",
      "Epoch 112/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1549 - acc: 0.9498\n",
      "Epoch 113/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1709 - acc: 0.9433\n",
      "Epoch 114/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1661 - acc: 0.9423\n",
      "Epoch 115/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1844 - acc: 0.9443\n",
      "Epoch 116/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1796 - acc: 0.9378\n",
      "Epoch 117/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1525 - acc: 0.9523\n",
      "Epoch 118/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1796 - acc: 0.9418\n",
      "Epoch 119/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2047 - acc: 0.9393\n",
      "Epoch 120/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2656 - acc: 0.9157\n",
      "Epoch 121/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1659 - acc: 0.9433\n",
      "Epoch 122/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1500 - acc: 0.9513\n",
      "Epoch 123/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1784 - acc: 0.9438\n",
      "Epoch 124/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2142 - acc: 0.9212\n",
      "Epoch 125/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1619 - acc: 0.9498\n",
      "Epoch 126/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1574 - acc: 0.9453\n",
      "Epoch 127/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1480 - acc: 0.9563\n",
      "Epoch 128/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1507 - acc: 0.9513\n",
      "Epoch 129/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1826 - acc: 0.9458\n",
      "Epoch 130/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1888 - acc: 0.9413\n",
      "Epoch 131/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2115 - acc: 0.9282\n",
      "Epoch 132/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1905 - acc: 0.9428\n",
      "Epoch 133/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1580 - acc: 0.9523\n",
      "Epoch 134/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2292 - acc: 0.9333\n",
      "Epoch 135/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1834 - acc: 0.9363\n",
      "Epoch 136/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1805 - acc: 0.9393\n",
      "Epoch 137/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2040 - acc: 0.9368\n",
      "Epoch 138/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1600 - acc: 0.9438\n",
      "Epoch 139/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2067 - acc: 0.9328A: 0s - loss: 0.2078 - acc: \n",
      "Epoch 140/200\n",
      "1993/1993 [==============================] - 10s 5ms/step - loss: 0.1736 - acc: 0.9453\n",
      "Epoch 141/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1928 - acc: 0.9303\n",
      "Epoch 142/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1791 - acc: 0.9418\n",
      "Epoch 143/200\n",
      "1993/1993 [==============================] - 9s 4ms/step - loss: 0.1580 - acc: 0.9463\n",
      "Epoch 144/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1696 - acc: 0.9428\n",
      "Epoch 145/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1944 - acc: 0.9378\n",
      "Epoch 146/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1936 - acc: 0.9333\n",
      "Epoch 147/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1908 - acc: 0.9353\n",
      "Epoch 148/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1945 - acc: 0.9383\n",
      "Epoch 149/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1719 - acc: 0.9513\n",
      "Epoch 150/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1773 - acc: 0.9373\n",
      "Epoch 151/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1792 - acc: 0.9393\n",
      "Epoch 152/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1701 - acc: 0.9408\n",
      "Epoch 153/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1618 - acc: 0.9438\n",
      "Epoch 154/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1506 - acc: 0.9523\n",
      "Epoch 155/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1903 - acc: 0.9353\n",
      "Epoch 156/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1637 - acc: 0.9483\n",
      "Epoch 157/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1974 - acc: 0.9368\n",
      "Epoch 158/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.3646 - acc: 0.8956\n",
      "Epoch 159/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1964 - acc: 0.9373\n",
      "Epoch 160/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1722 - acc: 0.9453\n",
      "Epoch 161/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1445 - acc: 0.9528\n",
      "Epoch 162/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1788 - acc: 0.9403\n",
      "Epoch 163/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1853 - acc: 0.9368\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1876 - acc: 0.9393\n",
      "Epoch 165/200\n",
      "1993/1993 [==============================] - 9s 4ms/step - loss: 0.2178 - acc: 0.9333\n",
      "Epoch 166/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1858 - acc: 0.9328\n",
      "Epoch 167/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1905 - acc: 0.9368\n",
      "Epoch 168/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1676 - acc: 0.9453\n",
      "Epoch 169/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2377 - acc: 0.9242\n",
      "Epoch 170/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2316 - acc: 0.9237\n",
      "Epoch 171/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1624 - acc: 0.9493\n",
      "Epoch 172/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1685 - acc: 0.9518\n",
      "Epoch 173/200\n",
      "1993/1993 [==============================] - 9s 4ms/step - loss: 0.2174 - acc: 0.9318\n",
      "Epoch 174/200\n",
      "1993/1993 [==============================] - 9s 5ms/step - loss: 0.1652 - acc: 0.9463A: 0s - loss: 0.1620 \n",
      "Epoch 175/200\n",
      "1993/1993 [==============================] - 9s 4ms/step - loss: 0.2547 - acc: 0.9192\n",
      "Epoch 176/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2044 - acc: 0.9333\n",
      "Epoch 177/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1803 - acc: 0.9383\n",
      "Epoch 178/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1892 - acc: 0.9383\n",
      "Epoch 179/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1686 - acc: 0.9408\n",
      "Epoch 180/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1743 - acc: 0.9428\n",
      "Epoch 181/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.2025 - acc: 0.9348\n",
      "Epoch 182/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1710 - acc: 0.9438\n",
      "Epoch 183/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1901 - acc: 0.9408\n",
      "Epoch 184/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1967 - acc: 0.9358\n",
      "Epoch 185/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1864 - acc: 0.9393\n",
      "Epoch 186/200\n",
      "1993/1993 [==============================] - 8s 4ms/step - loss: 0.1497 - acc: 0.9478\n",
      "Epoch 187/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1478 - acc: 0.9523\n",
      "Epoch 188/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2014 - acc: 0.9318\n",
      "Epoch 189/200\n",
      "1993/1993 [==============================] - 13s 6ms/step - loss: 0.1956 - acc: 0.9308\n",
      "Epoch 190/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1458 - acc: 0.9513\n",
      "Epoch 191/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1593 - acc: 0.9513\n",
      "Epoch 192/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1495 - acc: 0.9493\n",
      "Epoch 193/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1608 - acc: 0.9493\n",
      "Epoch 194/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1707 - acc: 0.9463\n",
      "Epoch 195/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.1856 - acc: 0.9378\n",
      "Epoch 196/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2207 - acc: 0.9293\n",
      "Epoch 197/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2486 - acc: 0.9257\n",
      "Epoch 198/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.2423 - acc: 0.9197\n",
      "Epoch 199/200\n",
      "1993/1993 [==============================] - 7s 3ms/step - loss: 0.2110 - acc: 0.9272\n",
      "Epoch 200/200\n",
      "1993/1993 [==============================] - 7s 4ms/step - loss: 0.1760 - acc: 0.9383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15064fdaa90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, Y, epochs = 200, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B_vhMwYZLM9_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      " : \n",
      "in all comparisons that rise in the heart the river with its multitudinous waves and its single current likens itself to a life whether of an individual or of a community . a man 's life is of many flashing moments and yet one stream a nation 's flows through all its citizens and yet is more than they . in such places one is aware with an almost insupportable and yet comforting certitude that both men and nations are hurried onwards to their ruin or ending as inevitably as this dark flood . some go down to it unreluctant and meet it like the river not without nobility . and as incessant as inevitable and as unavailing as the spray that hangs over the falls is the white cloud of human crying ... . with some such thoughts does the platitudinous heart win from the confusion and thunder of a niagara peace that the quietest plains or most stable hills can never give . it the falls from touts finally . the facility of the thunder . in a mile bound throwing is the stream home and breeding-place . all the rainbows of earth . the senses are battered and bewildered and the thunder and clergymen are . the the bold traveler who attempts the trip under the american falls sees when he dare open his eyes to anything tiny baby rainbows some four or five yards in span leaping from rock to rock among the foam and gamboling beside him barely out of hand 's reach as he goes . one i saw in that place was a complete circle such as i have never seen before and so near that i could put my foot on it . it is a terrifying journey beneath and behind the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "# start = 1\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "# for i in range(len(pattern)):\n",
    "#     print(vec_to_word[pattern[i][0]], end = ' ')\n",
    "print(\" : \")\n",
    "\n",
    "for i in range(300):\n",
    "    x = np.reshape(pattern, (1, len(pattern), k))\n",
    "    prediction = model.predict(x, verbose = 0)\n",
    "    index = np.argmax(prediction)\n",
    "#     print(index)\n",
    "    result = int_to_word[index]\n",
    "    print(result, end = ' ')\n",
    "    pattern.append(int_to_vec[index])\n",
    "    pattern = pattern[1 : len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h4TV6X-1LM6o"
   },
   "source": [
    "the face . farther away in the roar and hissing clouds of spray seem literally to slide down some invisible plane of air . beyond the foot of the falls the river is like a slipping floor of marble green with veins of dirty white made by the scum that was foam . it slides very quietly and slowly down for a mile or two sullenly exhausted . then it turns to a dull sage green and hurries more swiftly smooth and ominous . as the walls of the ravine close in trouble stirs and the waters boil and eddy . these are the lower rapids a sight more terrifying than the falls because less intelligible . close in its bands of rock the river surges tumultuously forward writhing and leaping as if inspired by a demon . it is pressed by the straits into a visibly convex form . great planes of water slide past . sometimes it is thrown up into a pinnacle of foam higher than a house or leaps with incredible speed from the crest of one vast wave to another along the shining curve between like the spring of a wild beast . its motion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZRmZMeGQLF-l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Copy of word prediction.ipynb",
   "provenance": [
    {
     "file_id": "1y_xpDWLX1mHv1VK8LG_5PUS5vaGBiecI",
     "timestamp": 1533155505937
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
